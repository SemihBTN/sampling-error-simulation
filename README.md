Ã–rnekleme SimÃ¼lasyonu: SayÄ±larÄ±n GÃ¼cÃ¼
HiÃ§ merak ettiniz mi; bir ankette 10 kiÅŸiyle konuÅŸmak neden yetmez de binlerce kiÅŸiye ulaÅŸmaya Ã§alÄ±ÅŸÄ±rÄ±z? Bu proje, "Daha fazla veri, daha az hata" ilkesini Python kullanarak gÃ¶rsel bir kanÄ±ta dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yor.

ğŸ” Neyi Ä°nceledim?
Ä°statistikte Basit Rastgele Ã–rnekleme yaparken, kÃ¼Ã§Ã¼k gruplar bizi yanÄ±ltabilir. Åans eseri hep uÃ§ deÄŸerleri (Ã§ok yaÅŸlÄ±lar veya Ã§ok zenginler gibi) seÃ§ebiliriz. Ben bu "ÅŸans faktÃ¶rÃ¼nÃ¼n" (varyans) veri boyutu arttÄ±kÃ§a nasÄ±l etkisiz hale geldiÄŸini simÃ¼le ettim.

ğŸ› ï¸ Ne YaptÄ±m? (Hokus Pokus KÄ±smÄ±)
Ham Veri: Ã–nce 10.000 kiÅŸilik hayali bir topluluk oluÅŸturdum.

ZikzaklarÄ± Silmek: Tek bir Ã§ekim yapÄ±p "tamam oldu" demedim. Her Ã¶rneklem boyutu iÃ§in 100 farklÄ± deneme yapÄ±p bunlarÄ±n ortalamasÄ±nÄ± aldÄ±m. Buna "gÃ¼rÃ¼ltÃ¼ temizleme" diyoruz.

SonuÃ§: Ortaya Ã§Ä±kan grafik, rastgeleliÄŸin iÃ§indeki o mÃ¼kemmel dÃ¼zeni; yani veri arttÄ±kÃ§a hatanÄ±n nasÄ±l sÄ±fÄ±ra sÃ¼zÃ¼ldÃ¼ÄŸÃ¼nÃ¼ gÃ¶steriyor.

ğŸ’¡ Bana Ne Ã–ÄŸretti?
Bu Ã§alÄ±ÅŸma sadece bir grafik deÄŸil; BÃ¼yÃ¼k SayÄ±lar YasasÄ±'nÄ±n (Law of Large Numbers) Ã§alÄ±ÅŸan bir Ã¶rneÄŸi. Veri biliminde "yeterli Ã¶rneklem" kavramÄ±nÄ±n neden hayati olduÄŸunu ve kÃ¼Ã§Ã¼k verilerle karar vermenin ne kadar riskli olduÄŸunu bu sayede koda dÃ¶kmÃ¼ÅŸ oldum.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


ğŸŒ (English Version)
ğŸ¯ Sampling Simulation: The Power of Numbers
Have you ever wondered why talking to 10 people in a survey isn't enough, but reaching thousands is? This project provides a visual proof of the "More data, less error" principle using Python.

ğŸ” What Did I Investigate?
In statistics, when using Simple Random Sampling, small groups can be misleading. By chance, we might only pick outliers (like only the very old or very wealthy). I simulated how this "chance factor" (variance) becomes insignificant as the sample size increases.

ğŸ› ï¸ What Did I Do? (The Magic Behind the Scenes)
Raw Data: I generated a synthetic population of 10,000 individuals.

Cleaning the Noise: I didn't just take a single sample and call it a day. For each sample size, I performed 100 different trials and calculated their average. This is known as "noise reduction."

The Result: The resulting graph shows the perfect order within randomness; specifically, how the error smoothly glides toward zero as the data grows.

ğŸ’¡ Key Takeaway
This study is more than just a chart; it is a working demonstration of the Law of Large Numbers. Through this simulation, Iâ€™ve coded the fundamental reason why "adequate sample size" is vital in data science and why making decisions based on small datasets is highly risky.


